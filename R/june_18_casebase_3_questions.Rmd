---
title: "Casebase June 18th (3 Questions)"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
library(casebase)
library(dotwhisker)
library(tidyr)
library(broom)
library(dplyr)
library(survival)
library(glmnet)
library(ncvreg)
library(stringr)
library(lubridate)
library(knitr)
library(riskRegression)
library(visreg)
library(c060)
```


```{r, echo = FALSE}
n = 1000
p = 20
rho = 0.5
snr = 3

set.seed(123)

# function to generate x
gen.x = function(n,p,rho){
 if(abs(rho)<1){
  beta=sqrt(rho/(1-rho))
 x0=matrix(rnorm(n*p),ncol=p)
 z=rnorm(n)
 x=beta*matrix(z,nrow=n,ncol=p,byrow=F)+x0
 }
 if(abs(rho)==1){ x=matrix(z,nrow=n,ncol=p,byrow=F)}

return(x)
}

# function to generate survival times
gen.times = function(x,snr){
n=nrow(x)
p=ncol(x)
b=((-1)^(1:p))*exp(-2*( (1:p)-1)/20)
f=x%*%b
e=rnorm(n)
k=sqrt(var(f)/(snr*var(e)))
y=exp(f+k*e)
return(y)
}

# function to generate censoring times
gen.times.censor = function(x,snr){
n=nrow(x)
p=ncol(x)
b=((-1)^(1:p))*exp(-2*( (1:p)-1)/20)
f=x%*%b
e=rnorm(n)
k=sqrt(var(f)/(snr*var(e)))
y=exp(k*e)
return(y)
}

set.seed(123)

# generate X's
x_df <- gen.x(n = n, p = p, rho = rho) %>% as.data.frame()

# generate true survival times
surv_times <- gen.times(x = as.matrix(x_df), snr = 3)

# generate censoring times
cens_times <- gen.times.censor(x = as.matrix(x_df), snr = 3)

# generate survival/censoring times
y_mat <- cbind(surv_times, cens_times)
d.time <- apply(y_mat, MARGIN = 1, FUN = min)

# generate censoring status (death)
death <- ifelse(cens_times <= surv_times, yes = 0, no = 1)

# combine generated data
df <- cbind(death, d.time, x_df)

# split into train and test: 80% train and 20% test
train_indices <- sample(x = nrow(df), size = 0.80*nrow(df)) # randomly sample 95% of observations
test_indices <- setdiff(x = 1:nrow(df), y = train_indices) # get the remaining 5% of observations

train_df <- df[train_indices,]
test_df <- df[test_indices,]

# train and test data wrangling

# train data
x_train <- model.matrix(~ . -death -d.time, data = train_df)[, -1] # turns characters into numbers 
y_train <- train_df %>% 
  select(death, d.time) %>%
  data.matrix()

y_train_2 <- with(data = y_train %>% as.data.frame(), expr = Surv(time = d.time, event = death)) # get times and censor status from train_df
# plus/0 is censored

# test data
x_test <- model.matrix(~ . -death -d.time, data = test_df)[, -1]
y_test <- test_df %>% 
  select(death, d.time) %>%
  data.matrix()

y_test_2 <- with(data = y_test %>% as.data.frame(), expr = Surv(time = d.time, event = death))

test_df_model <- cbind(y_test, x_test) %>% as.data.frame()

kable(head(df)[,1:7], caption = "The Generated Data")
```

```{r}
head(x_train)
```

```{r}
head(y_train)
```

```{r}
head(y_train_2)
```

```{r}
head(x_test)
```

## 1. Get the Casebase Concordance

How can we get the risk score predictions from the casebase model? The predict function in the casebase package is drawn from glmnet with family binomial.

The problem with the prediction function is that it asks for a newx of 21 variables when I have 20 X's.

```{r}
# (4) pen_cb (ridge)
ridge_pen_cb <- fitSmoothHazard.fit(x = x_train, y = y_train,
                    family = "glmnet",
                    time = "d.time",
                    event = "death",
                    formula_time = ~ log(d.time), # how hazard depends on time
                    alpha = 0, # ridge
                    ratio = 10, # ratio of size of base series to case series
                    standardize = TRUE,
                    penalty.factor = c(0, rep(1, ncol(x_train))) #0 and then number of 1's is number of columns in x
                    )

# predict(ridge_pen_cb, newx = x_test, type = "response")
x_test %>% ncol()

coef(ridge_pen_cb) %>% head()
```

predict adds log time in front of it, feed newx with log(d.time) in front. it's part of the logistic regression model.
it will give you a predicted risk score for a certain follow up time. 

what is the formula for the predicted probability? what's the cumulative incidence function for a given covariate profile? write out that formula, and double check that to get the CIF (F_hat (t)). F_hat (t) is given a time t and covariate X, what is your predicted prob.

### sampleCaseBase function

```{r}
# create a survival object from the dataset
data <- as.data.frame(cbind(y_train, x_train))
survObj <- Surv(data[["d.time"]],
     data$death,
     type = "right")

n <- nrow(survObj) # number of subjects
B <- sum(survObj[, "time"]) # sum of all the times from all subjects
c <- sum(survObj[, "status"]) # number of cases (sum of all failures 1's)
ratio = 10
b <- ratio * c # size of the base series
offset <- log(B/b)

# select person-moments from individuals proportional to their total follow-up time
prob_select <- survObj[, "time"]/B # all individual's time/total_time (the prob of selecting that individ's time out of all other individ's time)
which_pm <- sample(n, b, replace = TRUE, prob = prob_select) # sample b bases out of n subjects with replacement, each n having a unique prob_select 
bSeries <- as.matrix(survObj[which_pm,]) # get all times and status of all n subjects chosen (chosen with repeats)
bSeries[, "status"] <- 0 # turn all status' into 0's
bSeries[, "time"] <- bSeries[, "time"] * runif(b) # multiply all times with a random number from Unif(0, 1) distribution

# combine base series with covariate data
selectTimeEvent <- !(colnames(data) %in% c("d.time", "death")) # logical vector of TRUE if X col and FALSE if not
bSeries <- cbind(bSeries,
      subset(data, select = selectTimeEvent)[which_pm, ,
                                             drop = FALSE]) # combine base series time and status with corresponding X's from original data (use drop = FALSE to ensure it is a dataframe)

# rename columns
names(bSeries)[names(bSeries) == "status"] <- "death"
names(bSeries)[names(bSeries) == "time"] <- "d.time"

# get case series
cSeries <- data[which(subset(data, select = (names(data) == "death")) != 0),] # get all rows from original data with cases (which function returns the row positions of all the death = 1, then take all the original data columns of those rows)

# combine case and base series
cbSeries <- rbind(cSeries, bSeries)

# add offset to the data
cbSeries <- cbind(cbSeries, rep_len(offset, nrow(cbSeries))) # rep_length just replicates the one offset value for all the rows 
names(cbSeries)[ncol(cbSeries)] <- "offset" # rename offset col
class(cbSeries) <- c("cbData", class(cbSeries)) # set class to be both cbData and data.frame

head(cbSeries)
```


### fitSmoothHazard.fit using sampleCaseBase 

```{r}
# call sampleCaseBase
sampleData <- cbSeries # after running the sampleCaseBase function, it returns sampleData

# format everything into matrices and expand variables that need to be
sample_event <- as.matrix(sampleData[, "death"]) # get the death col of 0s and 1s from cbSeries
sample_time <- model.matrix(update(~ log(d.time), ~ . -1), sampleData) # output a column that is the log of d.time in sampleData (only do this if the family = "glmnet" or "gbm") # else, model.matrix(~ log(d.time), sampleData)
sample_time_x <- cbind(sample_time,
                       as.matrix(sampleData[, !names(sampleData) %in% c("death", "d.time", "offset")])) # get all the X col and add log(d.time) col
sample_offset <- sampleData$offset

# fit a binomial model (assume no competing risks) (cv.glmnet_offset_hack will do this)

# out <- cv.glmnet_offset_hack(sample_time_x, sample_event,
#                              family = "binomial",
#                              offset = sample_offset, ...) # fit this if glmnet, if glm: glm.fit(sample_time_x, sample_event, family = binomial(), offset = sample_offset)
```

### cv.glmnet_offset_hack

```{r}
offset <- sample_offset
# checks if the offset values are the same in the offset vector
if (diff(range(offset)) > 1e-06) {
    stop("Glmnet is only available with constant offset",
      call. = FALSE
    )
  }

offset_value <- unique(offset)[1]

# 1. Fit without offset
out <- cv.glmnet(x = sample_time_x, y = sample_event,
                 family = "binomial",
                 offset = sample_offset) # fit a binomial model (logistic regression) without offset

# 2. Fix the intercept
out$glmnet.fit$a0 <- out$glmnet.fit$a0 - offset_value # a0 returns a list of intercept values for each lambda value (strength of the penalty), we fix the intercept by subtracting all intercept values with the offset value

print(out)
```

### fitSmoothHazard.fit after cv.glmnet_offset_hack

```{r}
# adding additional arguments
out$originalData <- list(
  "x" = x_train,
  "y" = y_train
)
out$typeEvents <- sort(unique(y_train[, "death"]))
out$timeVar <- "d.time"
out$eventVar <- "death"
out$matrix.fit <- TRUE
out$formula_time <- ~ log(d.time)
out$offset <- sample_offset

# create new class
class(out) <- c("singleEventCB", class(out))

print(out)
```

```{r}
set.seed(124)
# sanity checks of logistic regression in cv.glmnet
data(BinomialExample)
x <- BinomialExample$x # x is all the X's
y <- BinomialExample$y # y is 0's or 1's

# reduce X to 2 columns
x_2 <- x[,1:2]

# fit a simple binomial model
fit <- cv.glmnet(x = x_2, y = y, family = "binomial")

# betas for linear predictors
coef(fit, s = "lambda.min")

# check predict link: compare the function to get linear predictors with manual method
predict(fit, newx = c(1, 2), type = "link", s = "lambda.min")
0.26169 + 0.3841829*2

# check predict response: compare the function to get the fitted probability of p_i or P(Y_i = 1) with manual method based off of linear pred
predict(fit, newx = c(1,2), type = "response", s = "lambda.min")
1/(1 + exp(-predict(fit, newx = c(1, 2), type = "link", s = "lambda.min")))

# check predict class: if fitted prob in predict response is > 0.5 it should be class 1
predict(fit, newx = c(1,2), type = "class", s = "lambda.min")
```

$$
\log\left\{\frac{P(Y_i=1)}{1-P(Y_i=1)}\right\} = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_pX_{ip}, \quad i = 1,2,\cdots, n
$$

where ${P(Y_i=1)}$ is also denoted as $p_i$.

$$
p_i = \frac{1}{1 + \exp{\{-\boldsymbol{X_i}\boldsymbol{\beta}\}}}
$$

## Solution: (1) Get the Casebase Concordance

How can we get the risk score predictions from the casebase model? The predict function in the casebase package is drawn from glmnet with family binomial.

The problem with the prediction function is that it asks for a newx of 21 variables when I have 20 X's.

```{r}
# (4) pen_cb (ridge)
ridge_pen_cb <- fitSmoothHazard.fit(x = x_train, y = y_train,
                    family = "glmnet",
                    time = "d.time",
                    event = "death",
                    formula_time = ~ log(d.time), # how hazard depends on time
                    alpha = 0, # ridge
                    ratio = 10, # ratio of size of base series to case series
                    standardize = TRUE,
                    penalty.factor = c(0, rep(1, ncol(x_train))) #0 and then number of 1's is number of columns in x
                    )

# solution: add log(d.time) as a first column to x_test
x_test_cb <- cbind(log(y_test[,"d.time"]), x_test)

# for each subject, this gives the probability that y=1 (death = 1)
predict(ridge_pen_cb, newx = x_test_cb, type = "response", s = "lambda.min") %>% head()
```

Now, get the concordance for a casebase model given that we can get the risk scores/predicted probs.

```{r, echo = FALSE}
# (1) concordance (my function)

# takes in a data frame with columns:
# pred: a column of predicted risk scores from a fitted model using x_test
# time: a column of the true survival time
# status: a column of the true status (censored = 0 or failure = 1)

conc <- function(df){
  row.names(df) <- NULL
  all_pairs <- combn(x = row.names(df), m = 2) %>% as.data.frame()
  
  # get evaluable pairs
  eval <- rep(NA, ncol(all_pairs))
  
  for (i in 1:ncol(all_pairs)) {
    x <- rbind(df[as.numeric(all_pairs[1,i]),],  df[as.numeric(all_pairs[2,i]),])
    if (all(c(1, 1) == x$status)) {
      eval[i] <- "E"
    } else if ((all(c(0, 1) == x$status)) & (x$time[1] > x$time[2])) {
      eval[i] <- "E"
    } else if ((all(c(1, 0) == x$status)) & (x$time[1] < x$time[2])) {
      eval[i] <- "E"
    } else {
      eval[i] <- "I"
    }

  }
  
  # get the class if pairs are evaluable
  class <- rep(NA, ncol(all_pairs))
  
  for (i in 1:ncol(all_pairs)) {
  x <- rbind(df[as.numeric(all_pairs[1,i]),],  df[as.numeric(all_pairs[2,i]),])
  
  if (eval[i] == "E"){
    
  if (((x$pred[1] > x$pred[2]) & (x$time[1] < x$time[2])) | ((x$pred[1] < x$pred[2]) & (x$time[1] > x$time[2]))) {
    class[i] <- "C"
  } else if (((x$pred[1] > x$pred[2]) & (x$time[1] > x$time[2])) | ((x$pred[1] < x$pred[2]) & (x$time[1] < x$time[2]))) {
    class[i] <- "D"
  } else if (x$pred[1] == x$pred[2]) {
    class[i] <- "R"
  } else if (x$time[1] == x$time[2]) {
    class[i] <- "T"
  }
    
  } else {
    class[i] <- "I"
  }
  }
  
  # count the number of C's, D's, and R's and put in C-index formula
  C <- str_count(class, "C") %>% sum()
  D <- str_count(class, "D") %>% sum()
  R <- str_count(class, "R") %>% sum()

  concordance = (C+(R/2))/(C+D+R)
  concordance
}

```


```{r}
y_testing <- cbind(y_test[,2], y_test[,1])
colnames(y_testing) <- c("time", "status")

# ridge_pen_cb concordance
pred <- predict(ridge_pen_cb, newx = x_test_cb, type = "response", s = "lambda.min")
df <- as.data.frame(cbind(pred, y_testing))
colnames(df) <- c("pred", "time", "status")
ridge_cb_conc <- conc(df)
ridge_cb_conc
```

Question: Should I use $P(Y_i = 1)$ as the risk score to compute concordance? Or should I use the linear predictor? I can get both.

Notes:


## 2. Figure out the Score function 

Why is it that the Score function (from riskRegression package) cannot take in a glmnet object? (Need this to get the Brier Score)

How does the Score function compute the Brier Score mathematically? (it's different from my method)


```{r}
# (2) pen_cox method (ridge)
ridge_pen_cox <- cv.glmnet(x = x_train, y = y_train_2, family = "cox", alpha = 0) # cv does cross validation, in this case it selects the lambda/model that maximizes the partial likelihood, because it is cox

# brier_ridge_pen_cox <- Score(list("Ridge Pen Cox" = ridge_pen_cox),
#       data = test_df,
#       formula = Hist(d.time, death != 0) ~ 1, summary = NULL,
#       se.fit = FALSE, metrics = "brier", contrasts = FALSE,
#       times = times)
```

time-dependent Brier Score somehow, not just a dichotomous outcome.

## 3. Getting the Brier Score for Penalized Cox Using My Method

How can we get prediction probabilities from a glmnet object with the predict function?

```{r}
# (2) pen_cox method (ridge)
ridge_pen_cox <- cv.glmnet(x = x_train, y = y_train_2, family = "cox", alpha = 0) # cv does cross validation, in this case it selects the lambda/model that maximizes the partial likelihood, because it is cox

pred <- predict(ridge_pen_cox, newx = x_test, s = "lambda.min", type = "response")
head(pred)
```

how to take the linear predictors to get the predicted probabilities. you need the CIF, which you get from the baseline hazard*e^(BX's). 

don't pass the variable, but you pass the coefficients
init = c("coef for every variable")
max_iter = ...

package to get predicted probs from glmnet:
https://www.rdocumentation.org/packages/c060/versions/0.2-9/topics/predictProb.coxnet

## Solution: (3) Getting the Brier Score for Penalized Cox Using My Method

How can we get prediction probabilities from a glmnet object with the predict function?

```{r}
# (2) pen_cox method (ridge)
ridge_pen_cox <- cv.glmnet(x = x_train, y = y_train_2, family = "cox", alpha = 0) # cv does cross validation, if we set s = "lambda.min", we are able to select the lambda that maximizes the partial likelihood (because it is cox) for predictions or coefficients

y_test_cox <- y_test[,c(2,1)]
colnames(y_test_cox) <- c("time", "status")

# predictProb(ridge_pen_cox, response = y_test_cox, x = x_test)
# 
# pred <- predict(ridge_pen_cox, newx = x_test, s = "lambda.min", type = "response")
# head(pred)

```

```{r}

# predictProb.coxnet <- predictProb.glmnet <- function (object, response, x, times, complexity,  ...) 
# {
#     #require(glmnet)    
#     lp       <- as.numeric(predict(object, newx=data.matrix(x),s=complexity, type="link"))
#     basesurv <- basesurv(object$response,object$linear.predictor, sort(unique(times)))
#     p        <- exp(exp(lp) %*% -t(basesurv$cumBaseHaz))
#     
#     if (NROW(p) != NROW(x) || NCOL(p) != length(times)) 
#         stop("Prediction failed")
#     p
# }
# 
# predictProb(ridge_pen_cox, response = y_test_cox, x = x_test)
# 
# predict(ridge_pen_cox, newx = x_test, s = "lambda.min", type = "link")
# 
# basesurv(ridge_pen_cox$y_test_cox, ridge_pen_cox$linear.predictor, sort(unique(times)))

```

The hazard for individual $i$ at time $t$ for a Cox model.

$$
h_i(t) = h_0(t)*exp(\beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_kx_{ik})
$$

Question: Mathematically, how do we get the survival probabilities given a Cox model (with Betas)? 

Notes: Clean everything and restart R, try predictProb.glmnet

Once you finish all the concordance and brier score. Finish the simulation and write about it and illustrate. By mid-july you should have that written out. When Gaby comes back, move to another simulation setting. 2 weeks for simulation, then 2 weeks for writing, then that will be mid-August and 2 weeks for cleaning and evaluating.

